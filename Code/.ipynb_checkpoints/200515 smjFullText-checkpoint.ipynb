{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose.\n",
    "\n",
    "Here we collect abstracts and titles from [*Strategic Management Journal*](https://onlinelibrary.wiley.com/journal/10970266), flagship publication of the [Strategic Management Society](https://www.strategicmanagement.net/).  This information is to be used as part of an attempt to apply text classification to charting the progression of business strategy.\n",
    "\n",
    "## Approach.\n",
    "\n",
    "Titles and abstracts are available without paywall login. But we have to do this in ***three stages***.  \n",
    "\n",
    "â“µ **Get the URL for the each volume**. We start with the journal's hope page.  Our essential information is embedded in a frame depicted below on the right-hand side of the page. \n",
    "\n",
    "â“¶ **Get a list of issue URLs**.  Each volume page contains thumbnail images of individual issues. These include URLs to the individual isses. \n",
    "\n",
    "â“· **Collect lists of titles**.  Follow each issue's URL to its issue table of contents. The tables of contents contain titles, as well as URLs to pages for individual articles.\n",
    "\n",
    "â“¸ **Collect abstracts**.  Abstracts are accessible from individual articles. We have to get the abstracts from these individual-article pages.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "import json, time, io, os, shutil, datetime, re, sys, codecs, zipfile\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "\n",
    "\n",
    "from io import StringIO\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.pdfparser import PDFParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use lots of list-comprehension, which drives requests.get operations.\n",
    "# We need to \"throttle\" these, so as to avoid the appearance of a DDoS\n",
    "# attack.  We accomplish this by a get_sleep function.  This function \n",
    "# executes a req.get operation, returning the result, with a one-second\n",
    "# delay.\n",
    "def sleep_get(url, headers):\n",
    "    time.sleep(np.random.uniform(low = 0.75,\n",
    "                                 high = 2.75))\n",
    "    return(req.get(url,\n",
    "                   headers = headers))\n",
    "#\n",
    "# We also require a Selenium-based version of sleep_get.  This differs\n",
    "# from the Requests-based version above, in that a separate step is \n",
    "# required for the webdriver to navigate to the specified url.\n",
    "def selenium_sleep_get(url):\n",
    "    time.sleep(np.random.uniform(low = 1.25,\n",
    "                                 high = 3.75))\n",
    "    driver.get(url)\n",
    "    return BeautifulSoup(driver.page_source,\n",
    "                         'lxml')\n",
    "\n",
    "\n",
    "#\n",
    "# Partition a list into a specified number of bins.  Our inputs\n",
    "# are:\n",
    "# â§ parted_list is the list to be partitioned;\n",
    "# â§ partition_counts specifies the number of bins into which\n",
    "#   parted_list is divided.\n",
    "# We produce an enumerated dictionary of the list partitions.\n",
    "def partition_list(parted_list, partition_counts):\n",
    "    parted_list = np.sort(np.array(parted_list))\n",
    "    partition_len = int(np.ceil(len(parted_list)/partition_counts))\n",
    "    partitions = [np.array(object = range(partition_len)) + part * partition_len\n",
    "                     for part in range(partition_counts)]\n",
    "    partitions[-1] = np.arange(start = partitions[-1][0],\n",
    "                               stop = parted_list.shape[0])\n",
    "    return dict(enumerate([list(parted_list[part])\n",
    "                             for part in partitions]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_url = 'https://www.strategicmanagement.net/'\n",
    "smj_url = 'https://onlinelibrary.wiley.com'\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "    }\n",
    "smj_html = sleep_get(smj_url + '/loi/10970266',\n",
    "                   headers = headers)\n",
    "smj_soup = BeautifulSoup(smj_html.content, 'lxml')\n",
    "smj_login = json.load(open('/Users/nahamlet/Documents/Uncertainty Research/society logins/smj_logon.JSON'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nahamlet/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: DeprecationWarning: use options instead of chrome_options\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# â“ª Initiate websession with society website in order to log in.\n",
    "options = webdriver.ChromeOptions()\n",
    "preferences = {'download.default_directory' : '../data/',\n",
    "               'download.prompt_for_download' : False,\n",
    "               'download.directory_upgrade' : True,\n",
    "               \"plugins.always_open_pdf_externally\": True,\n",
    "               'download.extensions_to_open' : 'applications/pdf',\n",
    "               'plugins.plugins_list' : [{'enabled' : False,\n",
    "                                          'name' : 'Chrome PDF Viewer'}]}\n",
    "options.add_experimental_option('prefs', preferences)\n",
    "#  \n",
    "driver = webdriver.Chrome(chrome_options = options)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ›‘ Sign into society website ğŸ›‘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.get(sms_url)\n",
    "time.sleep(np.random.uniform(low = 4.5,\n",
    "                             high = 6))\n",
    "driver.find_element_by_link_text('Log In').click()\n",
    "time.sleep(np.random.uniform(low = 2.5,\n",
    "                             high = 5))\n",
    "driver.find_element_by_id('username').send_keys(smj_login.get('username'))\n",
    "driver.find_element_by_id('password').send_keys(smj_login.get('password'))\n",
    "driver.find_element_by_id('loginSubmit').click()\n",
    "\n",
    "# â“µ Capture the volumes and their urls. \n",
    "#    â“ Navigate to the journal's homepage.  We need to do this in three steps\n",
    "#       after logging in.  Shortcuts appear to break session integrity.  \n",
    "#       Get the entire page source as a BeautifulSoup object.\n",
    "driver.get('https://www.strategicmanagement.net/smj/overview/overview')\n",
    "time.sleep(np.random.uniform(low = 1.25,\n",
    "                             high = 2.5))\n",
    "driver.get('https://www.strategicmanagement.net/wiley/smj')\n",
    "time.sleep(np.random.uniform(low = 1.25,\n",
    "                             high = 2.5))\n",
    "driver.get('https://onlinelibrary.wiley.com/loi/10970266')\n",
    "smj_soup = BeautifulSoup(driver.page_source,\n",
    "                         'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#    â“‘ Instantiate an empty dictionary within which to capture the volumes.\n",
    "smj_volumes = dict()\n",
    "#\n",
    "#    â“’ Add current-decade volumes to the dictionary.\n",
    "smj_volumes.update(\n",
    "        {vol.attrs.get('title') : {'href' : vol.attrs.get('href')}\n",
    "            for vol in smj_soup.find('div', {'class' : 'loi--aside__left'})\\\n",
    "                               .find('li', {'class' : 'active'})} )\n",
    "#\n",
    "#    â““ Prior-decade volumes are embedded in nested lists.  We get these\n",
    "#       through a two-level dictionary comprehension.  Add the resulting\n",
    "#       dictionary to our smj_volumes dictionary using the dictionary.update()\n",
    "#       method.\n",
    "smj_volumes.update(\n",
    "        {vol.find('a').attrs.get('title') : {'href' : vol.find('a').attrs.get('href')}\n",
    "             for decade in smj_soup.find('ul', {'class' : 'rlist loi__list'})\\\n",
    "                                   .find_all('li', {'class' : 'nested'})\n",
    "             for vol in decade.find_all('li')}  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only perform â“¶ and â“· if not previously done, as evidenced by\n",
    "# absence of JSON file in specified directory.  Otherwise, read in the JSON file.\n",
    "if os.path.isfile('../data/stratMgtJrnlFullText.json'):\n",
    "    smj_volumes = json.load(open('../data/stratMgtJrnlFullText.json'))\n",
    "else:\n",
    "    # â“¶ Cycle through the volumes, now, and add issue information for each.\n",
    "    for vol in smj_volumes.keys():\n",
    "        vol_soup = selenium_sleep_get(smj_url + smj_volumes.get(vol).get('href'))\n",
    "        smj_volumes.get(vol).update(\n",
    "            {'issues' : {issue.find('h4').text : {'href' : issue.find('h4').find('a').attrs.get('href'),\n",
    "                                                  'coverDate' : issue.find('div', {'class' : 'coverDate'}).text}\n",
    "                                                    for issue in vol_soup.find('ul', {'class' : 'rlist loi__issues'})\\\n",
    "                                                                     .find_all('li', {'class' : 'card clearfix'})}})\n",
    "    # â“· Scan through all issues for all volumes.  \n",
    "    #    â“ For each issue, assemble an  issue table of contents, with an entry for \n",
    "    #       each article.  The entry includes a title, list of authors, article url, \n",
    "    #       and a url for the pdf.\n",
    "    for (vol, vol_summy) in smj_volumes.items():\n",
    "        for (issue, issue_summy) in vol_summy.get('issues').items():\n",
    "            print((vol, issue))\n",
    "            issue_soup = selenium_sleep_get(smj_url + smj_volumes.get(vol).get('issues').get(issue).get('href'))\n",
    "            issue_summy.update(\n",
    "                {'issue_toc' : \n",
    "                 dict(enumerate([{'title' : item.find('a', {'class' : 'issue-item__title visitable'})\\\n",
    "                                                 .find('h2')\\\n",
    "                                                 .text\\\n",
    "                                                 .lower(),\n",
    "                                 'authors' : [author.text\\\n",
    "                                                    .replace('\\n ', '')\\\n",
    "                                                    .lower()\n",
    "                                                 for author in item.find_all('span', {'class' : 'author-style'})],\n",
    "                                   'href' : item.find('a', {'class' : 'issue-item__title visitable'}).attrs.get('href'),\n",
    "                                   'pdf_url' : item.find('a', {'title' : 'EPDF'}).attrs.get('href')}\n",
    "                                    for container in issue_soup.find('div', {'class' :  'table-of-content'})\\\n",
    "                                                               .find('div', {'class' : 'table-of-content'})\\\n",
    "                                                               .find_all('div', {'class' : 'card issue-items-container exportCitationWrapper'})\n",
    "                                    for item in container.find_all('div', {'class', 'issue-item'})\n",
    "                                    if item.find('a').find('h2').text.lower() not in ['erratum',\n",
    "                                                                                      'masthead',\n",
    "                                                                                      'announcement',\n",
    "                                                                                      'issue information']]))\n",
    "                }  \n",
    "            )\n",
    "#\n",
    "# â“¸ Export the dictionary produced during â“· to a JSON file on local storage.\n",
    "#    â“‘ Export the dictionary produced during â“· to a JSON file on local storage.\n",
    "with io.open('../data/stratMgtJrnlFullText.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(smj_volumes, \n",
    "              f, \n",
    "              ensure_ascii = False, \n",
    "              indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_idx = dict(enumerate([(vol, issue, article)\n",
    "                                for (vol, vol_issues) in smj_volumes.items()\n",
    "                                for (issue, issue_articles) in vol_issues.get('issues').items()\n",
    "                                for article in issue_articles.get('issue_toc').keys()]))\n",
    "# corpus_idx.update({art_key : {'art_tuple' : art_tuple,\n",
    "#                                'collected' : False}\n",
    "#                     for (art_key, art_tuple) in corpus_idx.items()})\n",
    "# with io.open('../data/strat_mgt_j_idx.json', 'w', encoding = 'utf-8') as f:\n",
    "#     json.dump(corpus_idx, \n",
    "#               f, \n",
    "#               ensure_ascii = False, \n",
    "#               indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â“¹ Acquire the full-text article. Most volumnes have the text in pdf format.\n",
    "#    We use that as our approach, therefore.  \n",
    "#    â“ Clean all pdfs out of the download directory so that we only have to\n",
    "#       worry about one pdf.\n",
    "def download_pdf_article(url):\n",
    "    download_dir = '/Users/nahamlet/Downloads'\n",
    "    os.chdir('/Users/nahamlet/Documents/GitHub/uncertainty-research/BizStratTopicAnalysis')\n",
    "    pdf_files = [file \n",
    "                 for file in os.listdir(download_dir)\n",
    "                 if 'pdf' in file]\n",
    "    for file in pdf_files:\n",
    "        os.remove(os.path.join(download_dir, file))\n",
    "    #\n",
    "    #    â“‘ Download the pdf-format full-text article. We use Selenium for this purpose.\n",
    "    #       This must be done in two steps, because the actual file-download trigger\n",
    "    #       is buried in a drop-down menu. We must go through this in order to get \n",
    "    #       the actual download. Also, we \"drill\" down to the actual triggers\n",
    "    #       in multiple steps, in order to avoid ambiguiities. We introduce a pause\n",
    "    #       in order for the session state to catch up.\n",
    "    driver.get(url)\n",
    "    time.sleep(np.random.uniform(low = 5,\n",
    "                                 high = 7.5))\n",
    "    driver.find_element_by_css_selector('div.navbar-download.dropdown-widget')\\\n",
    "          .find_element_by_css_selector('button.dropdown-trigger.btn').click()\n",
    "    time.sleep(np.random.uniform(low = 2.5,\n",
    "                                 high = 3.5))\n",
    "    driver.find_element_by_css_selector('div.navbar-download.dropdown-widget')\\\n",
    "          .find_element_by_css_selector('div.row.flex-container')\\\n",
    "          .find_element_by_css_selector('span.icon.material-icons').click()\n",
    "    time.sleep(np.random.uniform(low = 2.5,\n",
    "                                 high = 3.5))\n",
    "    driver.find_element_by_css_selector('div.navbar-download.dropdown-widget')\\\n",
    "          .find_element_by_css_selector('button.dropdown-trigger.btn').click()\n",
    "    #\n",
    "    #    â“’ Move the journal article to the data directory.\n",
    "    if os.path.isfile('./data/journal_article.pdf'):\n",
    "        os.remove('./data/journal_article.pdf')\n",
    "    if os.path.isfile('./data/journal_article.txt'):\n",
    "        os.remove('./data/journal_article.txt')\n",
    "    time.sleep(np.random.uniform(low = 15,\n",
    "                                 high = 25))\n",
    "    pdf_files = [file \n",
    "                 for file in os.listdir(download_dir)\n",
    "                 if 'pdf' in file][0]\n",
    "    shutil.move(os.path.join(download_dir, pdf_files),\n",
    "                './data/journal_article.pdf')\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='#0051ba'> Reference Notes: </color>\n",
    "\n",
    "#### <font color = '#2767ff'> Extracting pdf into text.</color>\n",
    "\n",
    "This Stack overflow [Extracting text from a PDF file using PDFMiner in python?](https://stackoverflow.com/questions/26494211/extracting-text-from-a-pdf-file-using-pdfminer-in-python) demonstrates the extraction of a multi-column pdf article into a coherent text string. The essential point here is that the `pdfminer` package seems to be the one for us.  We get our preferred pattern for extracting pdf into text from the [pdfminer.six](https://pdfminersix.readthedocs.io/en/latest/tutorial/composable.html) documentation.\n",
    "\n",
    "#### <font color = '#2767ff'>Detecting parenthetically-delimited substrings.</color>\n",
    "\n",
    "This stack overflow [Python regex: matching a parenthesis within parenthesis](https://stackoverflow.com/questions/5357460/python-regex-matching-a-parenthesis-within-parenthesis) contains patterns for finding parentheses pairs. Most of these will be convenient to remove reference citations from our full-text articles.  The second, containing the pattern\n",
    "\n",
    "    `re.findall(r'\\([^()]*\\)', jrnl_article_raw_concat)`\n",
    "\n",
    "appears particularly convenient.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_article_to_string():\n",
    "    #    â““ Extract pdf into text string.\n",
    "    #       â…°. We first use pdfminer to extract the article pdf into a string.\n",
    "    os.chdir('/Users/nahamlet/Documents/GitHub/uncertainty-research/BizStratTopicAnalysis')\n",
    "    output_string = StringIO()\n",
    "    with open('./data/journal_article.pdf', 'rb') as in_file:\n",
    "        parser = PDFParser(in_file)\n",
    "        doc = PDFDocument(parser)\n",
    "        rsrcmgr = PDFResourceManager()\n",
    "        device = TextConverter(rsrcmgr, output_string, laparams=LAParams())\n",
    "        interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "        for page in PDFPage.create_pages(doc):\n",
    "            interpreter.process_page(page)\n",
    "    article_string = output_string.getvalue()\n",
    "    #\n",
    "    #       â…±. This string is quite \"dirty\", with lots of editorial markings and other\n",
    "    #          junk.  We want to get rid of this. We first try to segment the document\n",
    "    #          using section demarcations as proxies for labels.  We take only the\n",
    "    #          substring between the 'INTRODUCTION' and the 'ACKNOWLEDGMENTS' \n",
    "    #          section headers.  If these are not present, we use alternative, \n",
    "    #          noisier substrings. We also check if the APPENDIX follows the\n",
    "    #          REFERENCES, in which case we concatenate the appendix onto our string.\n",
    "    if re.search('REFERENCES', article_string) is not None:\n",
    "        refs = re.search('REFERENCES', article_string)\n",
    "    else:\n",
    "        refs = None\n",
    "    if re.search('APPEND', article_string) is not None:\n",
    "        apdx = re.search('APPEND', article_string)\n",
    "    else:\n",
    "        apdx = None\n",
    "    if re.search('ACKNOWLEDGEMENTS', article_string) is not None:\n",
    "        ack = re.search('ACKNOWLEDGEMENTS', article_string)\n",
    "    else:\n",
    "        ack = None\n",
    "    if re.search('INTRODUCTION', article_string) is not None:\n",
    "        intro = re.search('INTRODUCTION', article_string)\n",
    "    else:\n",
    "        intro = None\n",
    "    article_start = intro.end() if intro is not None else 0\n",
    "    article_end = ((-1 if refs is None\n",
    "                       else refs.start()) if ack is None\n",
    "                                          else ack.start())\n",
    "    article_string_body = article_string[article_start:article_end]\n",
    "    if (apdx is not None) and (apdx.start() > refs.end()):\n",
    "        article_string_body = article_string_body + article_string[apdx.start():]\n",
    "    #\n",
    "    #       â…². We now clean up our string.  We first split the string into \n",
    "    #          a list of substrings, and filter out substrings containing \n",
    "    #          editorial markers. We then rejoin our list and remove unwanted\n",
    "    #          characters from the reassembled string.\n",
    "    art_string_body_clean = ' '.join([article_line.replace('\\n', ' ')\n",
    "                                                 .lower()\n",
    "                                        for article_line in article_string_body.split('\\n')\n",
    "                                        if (('Strategic Management Journal' not in article_line)\n",
    "                                                and ('Strat. Mgmt. J.' not in  article_line)\n",
    "                                                and ('Wiley' not in article_line)\n",
    "                                                and ('Final revision' not in article_line)\n",
    "                                                and ('\\n' != article_line)\n",
    "                                                and ('\\x0c' not in article_line)\n",
    "                                                and ('DOI:' not in article_line)\n",
    "                                                and (not bool(re.match('\\d+\\n', article_line)))\n",
    "                                                and (not bool(re.match('\\d+.\\d+\\n', article_line)))\n",
    "                                                and (not bool(re.match('âˆ’\\d+.\\d+\\n', article_line)))\n",
    "                                                and (len(article_line.replace('\\n', '')) > 2)\n",
    "                                                and ('âˆ—' not in article_line)\n",
    "                                                and ('â€ ' not in article_line)\n",
    "                                                and ('|' not in article_line)\n",
    "                                       )]).replace('- ', '')\\\n",
    "                                          .replace('â€œ', '')\\\n",
    "                                          .replace('â€', '')\\\n",
    "                                          .replace('.', '')\\\n",
    "                                          .replace(',', '')\\\n",
    "                                          .replace('â€¦', '')\\\n",
    "                                          .replace('*', '')\\\n",
    "                                          .replace(\"\\'\", '')\\\n",
    "                                          .replace('/', '')\\\n",
    "                                          .replace('â„â„', '')\\\n",
    "                                          .replace('<', '')\\\n",
    "                                          .replace('>', '')\\\n",
    "                                          .replace('~', '')\\\n",
    "                                          .replace(':', '')\\\n",
    "                                          .replace('--', '')\\\n",
    "                                          .replace('[', '')\\\n",
    "                                          .replace(']', '')\\\n",
    "                                          .replace('?', '')\\\n",
    "                                          .replace(\"'\", '')\\\n",
    "                                          .replace(\"â€˜\", '')\\\n",
    "                                          .replace(\"â€™\", '')\\\n",
    "                                          .replace('copyright', '')\\\n",
    "                                          .replace('Strategic Management Journal', '')\\\n",
    "                                          .replace('strategic management journal', '')\n",
    "    art_string_body_clean = re.sub('\\d',     # â‡š Remove Numbers\n",
    "                                   '', \n",
    "                                   re.sub(r'\\([^()]*\\)', \n",
    "                                          '', \n",
    "                                          art_string_body_clean))\n",
    "    art_string_body_clean = ' '.join([token   # â‡š Remove multiple spaces\n",
    "                                        for token in art_string_body_clean.split(' ')\n",
    "                                        if len(token) > 1])\n",
    "    return {'full_text' : art_string_body_clean,\n",
    "            'time_collected' : datetime.datetime.utcnow().strftime('%y-%m-%d, %H%M%SZ'),\n",
    "            'word_count' : len(art_string_body_clean.split(' '))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_full_text_to_json(art_key):\n",
    "    #    â“” Add full-text string to our json.\n",
    "    #       â…°. Extract the randomly selected article from the \n",
    "    #          corpus JSON file.\n",
    "    os.chdir('/Users/nahamlet/Documents/GitHub/uncertainty-research/BizStratTopicAnalysis')\n",
    "    smj_volumes = json.load(open('./data/stratMgtJrnlFullText.json'))\n",
    "    (vol, issue, article_idx) = corpus_idx.get(art_key).get('art_tuple')\n",
    "    article = smj_volumes.get(vol).get('issues').get(issue).get('issue_toc').get(article_idx)\n",
    "    print(((vol, issue, article_idx),\n",
    "           datetime.datetime.utcnow().strftime('%y-%m-%d, %H%M%SZ')))\n",
    "    #\n",
    "    #       â…±. Invoke locally-defined program `download_pdf_article` to download\n",
    "    #          the randomly-selected article to local storage.\n",
    "    download_pdf_article(article.get('pdf_url'))\n",
    "    #\n",
    "    #       â…². Invoke locally-defined program `pdf_article_to_string` convert\n",
    "    #           the pdf fiel to a \"clean\" full-text string. Append to the \n",
    "    #           corresponding JSON branch.\n",
    "    article.update(pdf_article_to_string())\n",
    "    #\n",
    "    #       â…³. Update our corpus index JSON to change the state of our \n",
    "    #          randomly-selected article to `collected`. Also record the\n",
    "    #          time collected.\n",
    "    corpus_idx.get(art_key).update({'collected' : True,\n",
    "                                    'time_collected' : article.get('time_collected')})\n",
    "    #\n",
    "    #       â…´. Save our corpus-index and corpus JSON to local storage.\n",
    "    with io.open('./data/stratMgtJrnlFullText.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(smj_volumes, \n",
    "                  f, \n",
    "                  ensure_ascii = False, \n",
    "                  indent = 4)\n",
    "    #\n",
    "    #       â…µ. Write a small JSON recording the last article collected and\n",
    "    #          the timestamp of its collection.\n",
    "    with io.open('./data/strat_mgt_j_time_check.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump({'article_tuple' : corpus_idx.get(art_key).get('art_tuple'),\n",
    "                   'time_collected' : article.get('time_collected')}, \n",
    "                  f, \n",
    "                  ensure_ascii = False, \n",
    "                  indent = 4)\n",
    "    #\n",
    "    #       â…¶. Push all of the changed objects up to a GitHub repository.\n",
    "#     ! git add .\n",
    "#     ! git commit -m 'strat mgt j full-text articles'\n",
    "#     ! git push origin master\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/nahamlet/Documents/GitHub/uncertainty-research/BizStratTopicAnalysis')\n",
    "\n",
    "with zipfile.ZipFile('./data/stratMgtJrnlFullText.json.zip') as myzip:\n",
    "    with myzip.open('stratMgtJrnlFullText.json') as myfile:\n",
    "        smj_volumes = json.load(myfile)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('/Users/nahamlet/Documents/GitHub/uncertainty-research/BizStratTopicAnalysis')\n",
    "#        â“• Scan through the corpus index `corpus_idx` and extract full-text articles.\n",
    "#           We employ a while loop to identify control and find a stopping point.\n",
    "#           For each article we save our results to local storage and upload changes\n",
    "#           to GitHub. This way we continuously save corpus-acquisition state\n",
    "#           and are able to pick up where we left off should our session be\n",
    "#           interrupted.\n",
    "#           â…°. Load the `corpus_idx` JSON from local storage.\n",
    "corpus_idx = json.load(open('./data/strat_mgt_j_idx.json'))\n",
    "#\n",
    "#           â…±. Filter the `corpus_idx` dictionary to obtain only those\n",
    "#              articles for which the `collected` status is False.\n",
    "corp_to_do = {idx_key : corp_item\n",
    "                for (idx_key, corp_item) in corpus_idx.items()\n",
    "                if not corp_item.get('collected')}\n",
    "#\n",
    "#           â…². Randomly select one article from the `corp_to_do` dictionary.\n",
    "art_key = np.random.choice(list(corp_to_do.keys())).item(0)\n",
    "#\n",
    "#           â…³. Invoke the `add_full_text_to_json` locally-defined function\n",
    "#              to add the full-text article to the JSON.  This\n",
    "#              also changes the state for our article in the `corpus_idx`\n",
    "#              dictionary to `collected` as True.\n",
    "add_full_text_to_json(art_key)\n",
    "#\n",
    "#           â…´. Write the updated `corpus_idx` JSON to local storage.\n",
    "with io.open('./data/strat_mgt_j_idx.json', 'w', encoding = 'utf-8') as f:\n",
    "    json.dump(corpus_idx, \n",
    "              f, \n",
    "              ensure_ascii = False, \n",
    "              indent = 4)\n",
    "#\n",
    "#           â…µ. We now cycle through `corpus_idx` until all articles are collected.\n",
    "control_counter = 0\n",
    "while len(corp_to_do) > 0:\n",
    "    control_counter += 1\n",
    "    time.sleep(np.random.uniform(low = 7.5,                      # â‡š Random sleep interval.\n",
    "                                 high = 15))\n",
    "    corpus_idx = json.load(open('./data/strat_mgt_j_idx.json'))  # â‡š â…°. Load `corpus_idx`\n",
    "    corp_to_do = {idx_key : corp_item                     \n",
    "                    for (idx_key, corp_item) in corpus_idx.items()\n",
    "                    if not corp_item.get('collected')}           # â‡š â…±. Filter `corpus_idx`\n",
    "    art_key = np.random.choice(list(corp_to_do.keys())).item(0)  # â‡š â…². Random article\n",
    "    try: \n",
    "        add_full_text_to_json(art_key)                           # â‡š â…µ. Article text to JSON\n",
    "    except:\n",
    "        print('failed ' + datetime.datetime.utcnow().strftime('%y-%m-%d, %H%M%SZ'))\n",
    "        driver.get('https://onlinelibrary.wiley.com/loi/10970266') # â‡š Safe page if download fails\n",
    "    with io.open('./data/strat_mgt_j_idx.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(corpus_idx, \n",
    "                  f, \n",
    "                  ensure_ascii = False, \n",
    "                  indent = 4)                                    # â‡š â…´. Save `corpus_idx`\n",
    "    with io.open('./data/strat_mgt_j_articles_done.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump({idx_key : corp_item                     \n",
    "                    for (idx_key, corp_item) in corpus_idx.items()\n",
    "                    if corp_item.get('collected')}, \n",
    "                  f, \n",
    "                  ensure_ascii = False, \n",
    "                  indent = 4)                                    # â‡š â…´. Save `corpus_idx`\n",
    "#     if control_counter >= 5:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘\n",
    "\n",
    "### Here, we try to reproduce [[Moung, 2019]](https://medium.com/@moungpeter/how-to-automate-downloading-files-using-python-selenium-and-headless-chrome-9014f0cdd196)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘â›”ï¸âŒğŸš«âŒâ›”ï¸ğŸ›‘\n",
    "\n",
    "### Work below is Requests-based.  Only gets titles, abstracts"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
