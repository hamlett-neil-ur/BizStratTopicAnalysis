{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose.\n",
    "\n",
    "Here we collect abstracts and titles from [*International Journal of Business Strategy*](https://ijbs-journal.org/IJBS-JOURNAL/).  This information is to be used as part of an attempt to apply text classification to charting the progression of business strategy.\n",
    "\n",
    "## Approach.\n",
    "\n",
    "Titles and abstracts are available without paywall login. But we have to do this in ***three stages***.  \n",
    "\n",
    "⓵ **Get the URL for the each volume**. We start with the journal's hope page.  Our essential information is embedded in a frame depicted below on the right-hand side of the page. \n",
    "\n",
    "⓶ **Get a list of issue URLs**.  Each volume page contains thumbnail images of individual issues. These include URLs to the individual isses. \n",
    "\n",
    "⓷ **Collect lists of titles**.  Follow each issue's URL to its issue table of contents. The tables of contents contain titles, as well as URLs to pages for individual articles.\n",
    "\n",
    "⓸ **Collect abstracts**.  Abstracts are accessible from individual articles. We have to get the abstracts from these individual-article pages.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import itertools as it\n",
    "import json\n",
    "import io\n",
    "from copy import deepcopy\n",
    "import datetime\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "# driver = webdriver.Chrome()\n",
    "# driver.get(\"https://www.nytimes.com\")\n",
    "\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use lots of list-comprehension, which drives requests.get operations.\n",
    "# We need to \"throttle\" these, so as to avoid the appearance of a DDoS\n",
    "# attack.  We accomplish this by a get_sleep function.  This function \n",
    "# executes a req.get operation, returning the result, with a one-second\n",
    "# delay.\n",
    "def sleep_get(url, headers):\n",
    "    time.sleep(np.random.uniform(low = 2,\n",
    "                                 high = 4))\n",
    "    return(req.get(url,\n",
    "                   headers = headers))\n",
    "#\n",
    "# Partition a list into a specified number of bins.  Our inputs\n",
    "# are:\n",
    "# ⧐ parted_list is the list to be partitioned;\n",
    "# ⧐ partition_counts specifies the number of bins into which\n",
    "#   parted_list is divided.\n",
    "# We produce an enumerated dictionary of the list partitions.\n",
    "def partition_list(parted_list, partition_counts):\n",
    "    parted_list = np.sort(np.array(parted_list))\n",
    "    partition_len = int(np.ceil(len(parted_list)/partition_counts))\n",
    "    partitions = [np.array(object = range(partition_len)) + part * partition_len\n",
    "                     for part in range(partition_counts)]\n",
    "    partitions[-1] = np.arange(start = partitions[-1][0],\n",
    "                               stop = parted_list.shape[0])\n",
    "    return dict(enumerate([list(parted_list[part])\n",
    "                             for part in partitions]))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use a Selenium-driver session to get at the contents of the directory\n",
    "# page.  We find heterogeneity in the structure of the html.  It turns out that\n",
    "# content prior to about 2012 is in pdf format, threfore inaccessible using this\n",
    "# approach.\n",
    "# https://ijbs-journal.org/IJBS-JOURNAL/\n",
    "ijbsUrl = 'https://ijbs-journal.org/'\n",
    "ijbsSession = webdriver.Chrome()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our issue Tables of Contents are accessed through javascript click-through links.\n",
    "# We want to get all of these.  We use `BeautifulSoup` to do this.  These are demarcated\n",
    "# by html tags `<input type=\"submit\", name=\"«name»\", id=\"«id»\">`.  There are many of these\n",
    "# for various purposes.  We identify those that interest us in that they have values \n",
    "# with the volume, issue, publication date.\n",
    "issueVolSoup = {inputTag.attrs.get('value') : \n",
    "                         {'name' : inputTag.attrs.get('name'),\n",
    "                          'id' : inputTag.attrs.get('id')}\n",
    "                                    for inputTag in BeautifulSoup(sleep_get(ijbsUrl,\n",
    "                                                                                headers = headers).content,\n",
    "                                                                      'lxml').find_all('input', {'type' : 'submit'})\n",
    "                                    if 'Volume' in inputTag.attrs.get('value')}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue : Volume 18, Issue 1, Published in June 2018\n"
     ]
    }
   ],
   "source": [
    "issue = np.random.choice(a = list(issueVolSoup.keys()),\n",
    "                         size = 1).item(0)\n",
    "print(f'Issue : {issue}')\n",
    "issueTagName = issueVolSoup.get(issue).get('name')\n",
    "\n",
    "\n",
    "\n",
    "# We need to click through to each issue Table of Contents.  This works only if we \n",
    "# first return to the volume/issue directory.  This directory is accessed via a menu item\n",
    "# at the top of the home page demarcated by html tag\n",
    "#            `<button type=\"button\" id=\"changeTabTodiv_TAB_ArchiveFromTopMenu\">`.  \n",
    "# Once we've navigated back to this location we can click forward to an issue\n",
    "# table-of-contents page.\n",
    "def navigateToIssueToc(issueTagName):\n",
    "    # ⓵ Return to the journal's homepage.\n",
    "    ijbsSession.get(ijbsUrl)\n",
    "    time.sleep(np.random.uniform(low = 2,\n",
    "                                 high = 4))\n",
    "    #\n",
    "    # ⓶ \"Click\" the tab pointing to the archive section.\n",
    "    ijbsSession.find_element_by_id('changeTabTodiv_TAB_ArchiveFromTopMenu').click()\n",
    "    #\n",
    "    # ⓷ Navigate to the issue Table-of-contents page.\n",
    "    time.sleep(np.random.uniform(low = 2,\n",
    "                                 high = 4))\n",
    "    ijbsSession.find_element_by_name(issueVolSoup.get(issue).get('name')).click()\n",
    "    pass\n",
    "#\n",
    "# The article information is available from a clean html-format\n",
    "# website.  We get all of our information from this site.\n",
    "def getArticleTitleAbstract(articleUrl):\n",
    "    articleSoup = BeautifulSoup(sleep_get(articleUrl,\n",
    "                                          headers = headers).content,\n",
    "                                'lxml')\n",
    "    issueToc = {'title' : articleSoup.find('span', {'id' : 'lbl_Title'})\\\n",
    "                                      .text\\\n",
    "                                      .lower(),\n",
    "                 'abstract' : articleSoup.find('span', {'id' : 'lbl_ABSTRACT'})\\\n",
    "                                         .text\\\n",
    "                                         .lower(),\n",
    "                  'pubDate' : articleSoup.find('span', {'id' : 'lbl_IssuePageDate'})\\\n",
    "                                         .text.split(', ')\\\n",
    "                                         [-1],\n",
    "                  'collected' : datetime.datetime.utcnow().strftime('%y-%m-%d, %H%M%SZ')}\n",
    "    try:\n",
    "        issueToc.update({'keywords' : [keyword.lower()\n",
    "                            for keyword in articleSoup.find('span', {'id' : 'lbl_Keywords'}).text\\\n",
    "                                                                              .split('; ')]})\n",
    "    except:\n",
    "        issueToc.update({'keywords' : list()})\n",
    "    return issueToc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This particular website is kind of kludgey, to state things in diplomatic terms.  We need `selenium` to parse the javascript. `BeautifulSoup` helps us with the html, once we've stripped it out.  \n",
    "\n",
    "In this case, our issue information is tabulated in tables on issue table-of-contents pages. We want to use `selenium` to get table itself.  This excises the table out of lots of clutter.  We then use `BeautifulSoup` for the table itself, because it gives better transparency.\n",
    "\n",
    "Here is the documentation for the [Selenium Client Driver](https://www.selenium.dev/selenium/docs/api/py/index.html).  This [`selenium.webdriver.remote.webelement`](https://www.selenium.dev/selenium/docs/api/py/webdriver_remote/selenium.webdriver.remote.webelement.html) gives us the web-element attributes.  A somewhat-old StackOverflow posting [Get HTML Source of WebElement in Selenium WebDriver using Python](https://stackoverflow.com/questions/7263824/get-html-source-of-webelement-in-selenium-webdriver-using-python) points us to the `webelement.get_attribute('innerHTML)` method, which gives us html content parsable using `BeautifulSoup`.\n",
    "\n",
    "So, the inner-most object encapsulating the the table of contents is an html object tagged `<div id=\"GridViewArchiveIssueArticles\">`.  We use `selenium` to get `webelement`.  The `.get_attribute('innerHTML')` method give a `BeautifulSoup`-parsable html block.\n",
    "\n",
    "Moving outsid-in, we get a `<tbody>`-tagged object and then use the `BeautifulSoup().find_all('tr')` to get the individual article \n",
    "entries. The unambiguous html elements demarcating individual articles in the table of contents are `<td valign=\"top\">`.  We use the `BeautifulSoup().find_all()` method to get these items.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issue : Volume 17, Issue 2, Published in October 2017\n",
      "Issue Volume 19, Issue 1, Published in March 2019, 10 articles, success at 20-03-05, 012507Z\n",
      "Issue Volume 18, Issue 2, Published in October 2018, 11 articles, success at 20-03-05, 012602Z\n",
      "Issue Volume 18, Issue 1, Published in June 2018, 7 articles, success at 20-03-05, 012638Z\n",
      "Issue Volume 18, Issue 1, Published in March 2018, 7 articles, success at 20-03-05, 012723Z\n",
      "Issue Volume 17, Issue 3, Published in December 2017, 7 articles, success at 20-03-05, 012803Z\n",
      "Issue Volume 17, Issue 2, Published in October 2017, 10 articles, success at 20-03-05, 012853Z\n",
      "Issue Volume 17, Issue 1, Published in March 2017, 7 articles, success at 20-03-05, 012946Z\n",
      "Issue Volume 16, Issue 2, Published in October 2016, 9 articles, success at 20-03-05, 013044Z\n",
      "Issue Volume 16, Issue 1, Published in March 2016, 6 articles, success at 20-03-05, 013129Z\n",
      "Issue Volume 15, Issue 3, Published in October 2015, 9 articles, success at 20-03-05, 013229Z\n",
      "Issue Volume 15, Issue 2, Published in June 2015, 6 articles, success at 20-03-05, 013314Z\n",
      "Issue Volume 15, Issue 1, Published in March 2015, 8 articles, success at 20-03-05, 013410Z\n",
      "Issue Volume 14, Issue 3, Published in October 2014, 19 articles, success at 20-03-05, 013549Z\n",
      "Issue Volume 14, Issue 2, Published in June 2014, 10 articles, success at 20-03-05, 013642Z\n",
      "Issue Volume 14, Issue 1, Published in March 2014, 13 articles, success at 20-03-05, 013749Z\n",
      "Issue Volume 13, Issue 4, Published in October 2013, 15 articles, success at 20-03-05, 013914Z\n",
      "Issue Volume 13, Issue 3, Published in October 2013, 11 articles, success at 20-03-05, 014011Z\n",
      "Issue Volume 13, Issue 2, Published in June 2013, 9 articles, success at 20-03-05, 014115Z\n",
      "Issue Volume 13, Issue 1, Published in March 2013, 12 articles, success at 20-03-05, 014225Z\n"
     ]
    }
   ],
   "source": [
    "# We now cycle through the issues and add the table-of-contents information, including\n",
    "# titles and abstracts. For each issue we:\n",
    "issue = np.random.choice(a = list(issueVolSoup.keys()),\n",
    "                         size = 1).item(0)\n",
    "print(f'Issue : {issue}')\n",
    "issueArticleTaken = list()\n",
    "issueArticleMissed = list()\n",
    "for issue in issueVolSoup.keys():\n",
    "    # ⓐ Nagigate to its ToC page.\n",
    "    navigateToIssueToc(issueVolSoup.get(issue).get('name'))\n",
    "    #\n",
    "    # ⓑ Get the html of which the tabulate issue table of contents is comprised.\n",
    "    #    We use `selenium` `webelement.get_attribute('innerHTML')` method to get\n",
    "    #    the html, which we then parse using BeautifulSoup.  We construct an\n",
    "    #    enumerated dictionary `issueToc` containing what we need.  All we get from this\n",
    "    #    💩💩💩 page is the URL for the articles themselves.\n",
    "    issueTocHtml = BeautifulSoup(ijbsSession.find_element_by_id('GridViewArchiveIssueArticles')\\\n",
    "                                            .get_attribute('innerHTML'),\n",
    "                                 'lxml').find('tbody')\\\n",
    "                                        .find_all('td', {'valign' : 'top'})\n",
    "    issueToc = dict(enumerate([{'href' : article.find('a')\\\n",
    "                                                .attrs\\\n",
    "                                                .get('href')}\n",
    "                                    for article in issueTocHtml ]))\n",
    "    #\n",
    "    # ⓒ We finally cycle through  the articles in issueToC.  We emply the previously-defined\n",
    "    #   `getArticleTitleAbstract` to fill out the remaining information in the enumerated\n",
    "    #   dictionary `issueToc`. \n",
    "    for article in issueToc.keys():\n",
    "        issueToc.get(article).update(getArticleTitleAbstract(issueToc.get(article).get('href')))\n",
    "    #\n",
    "    # ⓓ Finally, we update our original `issueVolumeSoup` dictionary with the\n",
    "    #    issueToc dictionary.\n",
    "    issueVolSoup.get(issue).update({'issueToc' : issueToc})\n",
    "    print('Issue {}, {} articles, success at {}'\\\n",
    "          .format(issue, \n",
    "                  len(issueToc), \n",
    "                  datetime.datetime.utcnow().strftime('%y-%m-%d, %H%M%SZ')) )\n",
    "    with io.open('../data/intJrnlBizStrat.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(issueVolSoup, \n",
    "                  f, \n",
    "                  ensure_ascii = False, \n",
    "                  indent = 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
