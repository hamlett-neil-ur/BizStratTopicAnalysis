{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose.\n",
    "\n",
    "Here we collect abstracts and titles from [*Strategic Management Journal*](https://onlinelibrary.wiley.com/journal/10970266), flagship publication of the [Strategic Management Society](https://www.strategicmanagement.net/).  This information is to be used as part of an attempt to apply text classification to charting the progression of business strategy.\n",
    "\n",
    "## Approach.\n",
    "\n",
    "Titles and abstracts are available without paywall login. But we have to do this in ***three stages***.  \n",
    "\n",
    "⓵ **Get the URL for the each volume**. We start with the journal's hope page.  Our essential information is embedded in a frame depicted below on the right-hand side of the page. \n",
    "\n",
    "⓶ **Get a list of issue URLs**.  Each volume page contains thumbnail images of individual issues. These include URLs to the individual isses. \n",
    "\n",
    "⓷ **Collect lists of titles**.  Follow each issue's URL to its issue table of contents. The tables of contents contain titles, as well as URLs to pages for individual articles.\n",
    "\n",
    "⓸ **Collect abstracts**.  Abstracts are accessible from individual articles. We have to get the abstracts from these individual-article pages.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libaries\n",
    "import requests as req\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import itertools as it\n",
    "import json\n",
    "import io\n",
    "from copy import deepcopy\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use lots of list-comprehension, which drives requests.get operations.\n",
    "# We need to \"throttle\" these, so as to avoid the appearance of a DDoS\n",
    "# attack.  We accomplish this by a get_sleep function.  This function \n",
    "# executes a req.get operation, returning the result, with a one-second\n",
    "# delay.\n",
    "def sleep_get(url, headers):\n",
    "    time.sleep(np.random.uniform(low = 0.75,\n",
    "                                 high = 2.75))\n",
    "    return(req.get(url,\n",
    "                   headers = headers))\n",
    "#\n",
    "# Partition a list into a specified number of bins.  Our inputs\n",
    "# are:\n",
    "# ⧐ parted_list is the list to be partitioned;\n",
    "# ⧐ partition_counts specifies the number of bins into which\n",
    "#   parted_list is divided.\n",
    "# We produce an enumerated dictionary of the list partitions.\n",
    "def partition_list(parted_list, partition_counts):\n",
    "    parted_list = np.sort(np.array(parted_list))\n",
    "    partition_len = int(np.ceil(len(parted_list)/partition_counts))\n",
    "    partitions = [np.array(object = range(partition_len)) + part * partition_len\n",
    "                     for part in range(partition_counts)]\n",
    "    partitions[-1] = np.arange(start = partitions[-1][0],\n",
    "                               stop = parted_list.shape[0])\n",
    "    return dict(enumerate([list(parted_list[part])\n",
    "                             for part in partitions]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smj_url = 'https://onlinelibrary.wiley.com'\n",
    "headers = {\n",
    "    'User-Agent' : 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36'\n",
    "    }\n",
    "smj_html = sleep_get(smj_url + '/loi/10970266',\n",
    "                   headers = headers)\n",
    "smj_soup = BeautifulSoup(smj_html.content, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://onlinelibrary.wiley.com/loi/10970266'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smj_url + '/loi/10970266'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ⓵ Obtain the URLs for each volume. These located on the journal's homepage.\n",
    "#    collect this in a dictionary labeled `smj_volumes`.  This is a simple \n",
    "#    nested dictionary of {'volume label ' : {'href' : vol_url}}.\n",
    "#    The vol_url attribute is appended to our smj url to get the\n",
    "#    URL for the issue table of contents.\n",
    "smj_volumes = {issues.find('a').attrs.get('title') : {'href' : issues.find('a').attrs.get('href')}\n",
    "                                                     for issues in smj_soup.find('div', {'class' : \"loi--aside__left\"})\\\n",
    "                                                                           .find_all('li')\n",
    "                                                     if issues.find('a').attrs.get('href') != '#' }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2020 - Volume 41': {'href': '/loi/10970266/year/2020'},\n",
       " '2019 - Volume 40': {'href': '/loi/10970266/year/2019'},\n",
       " '2018 - Volume 39': {'href': '/loi/10970266/year/2018'},\n",
       " '2017 - Volume 38': {'href': '/loi/10970266/year/2017'},\n",
       " '2016 - Volume 37': {'href': '/loi/10970266/year/2016'},\n",
       " '2015 - Volume 36': {'href': '/loi/10970266/year/2015'},\n",
       " '2014 - Volume 35': {'href': '/loi/10970266/year/2014'},\n",
       " '2013 - Volume 34': {'href': '/loi/10970266/year/2013'},\n",
       " '2012 - Volume 33': {'href': '/loi/10970266/year/2012'},\n",
       " '2011 - Volume 32': {'href': '/loi/10970266/year/2011'},\n",
       " '2010 - Volume 31': {'href': '/loi/10970266/year/2010'},\n",
       " '2009 - Volume 30': {'href': '/loi/10970266/year/2009'},\n",
       " '2008 - Volume 29': {'href': '/loi/10970266/year/2008'},\n",
       " '2007 - Volume 28': {'href': '/loi/10970266/year/2007'},\n",
       " '2006 - Volume 27': {'href': '/loi/10970266/year/2006'},\n",
       " '2005 - Volume 26': {'href': '/loi/10970266/year/2005'},\n",
       " '2004 - Volume 25': {'href': '/loi/10970266/year/2004'},\n",
       " '2003 - Volume 24': {'href': '/loi/10970266/year/2003'},\n",
       " '2002 - Volume 23': {'href': '/loi/10970266/year/2002'},\n",
       " '2001 - Volume 22': {'href': '/loi/10970266/year/2001'},\n",
       " '2000 - Volume 21': {'href': '/loi/10970266/year/2000'},\n",
       " '1999 - Volume 20': {'href': '/loi/10970266/year/1999'},\n",
       " '1998 - Volume 19': {'href': '/loi/10970266/year/1998'},\n",
       " '1997 - Volume 18': {'href': '/loi/10970266/year/1997'},\n",
       " '1996 - Volume 17': {'href': '/loi/10970266/year/1996'},\n",
       " '1995 - Volume 16': {'href': '/loi/10970266/year/1995'},\n",
       " '1994 - Volume 15': {'href': '/loi/10970266/year/1994'},\n",
       " '1993 - Volume 14': {'href': '/loi/10970266/year/1993'},\n",
       " '1992 - Volume 13': {'href': '/loi/10970266/year/1992'},\n",
       " '1991 - Volume 12': {'href': '/loi/10970266/year/1991'},\n",
       " '1990 - Volume 11': {'href': '/loi/10970266/year/1990'},\n",
       " '1989 - Volume 10': {'href': '/loi/10970266/year/1989'},\n",
       " '1988 - Volume 9': {'href': '/loi/10970266/year/1988'},\n",
       " '1987 - Volume 8': {'href': '/loi/10970266/year/1987'},\n",
       " '1986 - Volume 7': {'href': '/loi/10970266/year/1986'},\n",
       " '1985 - Volume 6': {'href': '/loi/10970266/year/1985'},\n",
       " '1984 - Volume 5': {'href': '/loi/10970266/year/1984'},\n",
       " '1983 - Volume 4': {'href': '/loi/10970266/year/1983'},\n",
       " '1982 - Volume 3': {'href': '/loi/10970266/year/1982'},\n",
       " '1981 - Volume 2': {'href': '/loi/10970266/year/1981'},\n",
       " '1980 - Volume 1': {'href': '/loi/10970266/year/1980'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smj_volumes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ⓶ Get the tables of contents for each issue of each volumne. We add this to\n",
    "#    our smj_volumes dictionary so that our dictionary look like\n",
    "#      {'volumne label' : {'href' : vol_url,\n",
    "#                          'issues' : {'issue label' : 'issue ToC URL'}}}.\n",
    "#    For compactness of logic, we construct a function `get_issue_tocs`\n",
    "#    to get the actual tables of contents.  We then invoke this attribute \n",
    "#    via dictionary comprehension to get \n",
    "def get_issue_tocs(issue_url):\n",
    "    # First we turn the issue URL into a soup object, using our sleep_get \n",
    "    # locally-defined function.\n",
    "    issue_soup = BeautifulSoup(sleep_get(smj_url + issue_url,\n",
    "                                          headers = headers).content,\n",
    "                               'lxml')\n",
    "    #\n",
    "    # We next get to enumerated dictionaries.  The first contains all of the \n",
    "    # cover-date instances.  These are demarcated by a <span>-tagged object,\n",
    "    # for which the `cover-date-value` class is assigned.  Secondly, we\n",
    "    # retrieve all instances of <h4>-tagged objects with a `parent-item`\n",
    "    # class label. These contain the issue information as well as the url to its ToC.\n",
    "    cover_date = dict(enumerate(issue_soup.find_all('span', {'class' : 'cover-date-value'})))\n",
    "    issue_href = dict(enumerate(issue_soup.find_all('h4', {'class' : 'parent-item'})))\n",
    "    #\n",
    "    # Now, merge the two dictionaries. In the process, we have to turn the issue_href\n",
    "    # inside-out.  We ant the text item, the issue information, to be our dictionary keys.\n",
    "    # The url to the issue ToC is an attribute of a <a>-tagged item. \n",
    "    return\\\n",
    "    {issue_attrs.text : {'href': issue_attrs.find('a').attrs.get('href'),\n",
    "                         'cover_date' : cover_date.get(issue_idx).text}\n",
    "                    for (issue_idx, issue_attrs) in issue_href.items()}\n",
    "\n",
    "\n",
    "\n",
    "smj_volumes.update({jrnl_vol : {'href' : vol_contents.get('href'),\n",
    "                                 'issues' : get_issue_tocs(vol_contents.get('href'))}\n",
    "                        for (jrnl_vol, vol_contents) in smj_volumes.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_abstract(abstract_url):\n",
    "    # We get the article abstracts given the URL on which they are located.\n",
    "    # We obtain these from the title_abstract_url object produced internal \n",
    "    # to the get_article_title_abst_url locally-defined function. \n",
    "    # Given this we get the html from the abstract page.  \n",
    "    #\n",
    "    # The abstract itself is embedded in a <div>-tagged object to which\n",
    "    # the label\n",
    "    #          `article-section__content en main`\n",
    "    # is applied.  The text of this contains our abstract itself.\n",
    "    # We apply some cleanup, removing new-line tags and \n",
    "    # summary headers.\n",
    "    abstract_soup = \\\n",
    "    BeautifulSoup(sleep_get(smj_url + abstract_url,\n",
    "                            headers = headers).content,\n",
    "                  'lxml')\n",
    "    abstract = abstract_soup.find('div', {'class' : 'article-section__content en main'})\\\n",
    "                            .text\\\n",
    "                            .replace('\\n', ' ')\\\n",
    "                            .replace('research summary:', ' ')\\\n",
    "                            .replace('managerial summary:', ' ')\\\n",
    "                            .strip()\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume : 2019 - Volume 40,   Issue : Volume 40, Issue 1\n",
      "https://onlinelibrary.wiley.com/toc/10970266/2019/40/1\n"
     ]
    }
   ],
   "source": [
    "volume = np.random.choice(a = list(smj_volumes.keys()),\n",
    "                          size = 1).item(0)\n",
    "issue = np.random.choice(a = list(smj_volumes.get(volume).get('issues').keys()),\n",
    "                         size = 1).item(0)\n",
    "print(f'Volume : {volume},   Issue : {issue}')\n",
    "\n",
    "smj_volumes.get(volume).get('issues').get(issue)\n",
    "\n",
    "issue_attrs = smj_volumes.get(volume).get('issues').get(issue)\n",
    "issue_url = issue_attrs.get('href')\n",
    "print(smj_url + issue_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"issue-item\">\n",
       "<div class=\"pull-left\">\n",
       "<div class=\"doi-access-wrapper\"><div class=\"free-access access-type\"> <i aria-hidden=\"true\" class=\"icon-icon-lock_open\"></i> <div class=\"doi-access\"> <div class=\"doi-access\" tabindex=\"0\">Free Access</div></div></div></div>\n",
       "</div>\n",
       "<div class=\"clearfix\"></div>\n",
       "<div class=\"clearfix\"></div>\n",
       "<div class=\"input-group exportCitationCheckbox js--hidden\"><label class=\"checkbox--primary\"><input data-type=\"exportCitation\" name=\"exportCitation\" title=\"Select article for bulk download or export: Multinational investment and the value of growth options: Alignment of incremental strategy to environmental uncertainty\" type=\"checkbox\" value=\"10.1002/smj.2969\"/><span class=\"label-txt\"></span></label></div><a class=\"issue-item__title visitable\" href=\"/doi/10.1002/smj.2969\">\n",
       "<h2>Multinational investment and the value of growth options: Alignment of incremental strategy to environmental uncertainty</h2></a><ul class=\"rlist--inline loa comma loa-authors-trunc\" style=\"float: none; position: static; list-style:none;\">\n",
       "<li><a href=\"/action/doSearch?ContribAuthorStored=Belderbos%2C+Ren%C3%A9\" title=\"René Belderbos\"><span class=\"author-style\">\n",
       " René Belderbos</span></a></li>\n",
       "<li><a href=\"/action/doSearch?ContribAuthorStored=Tong%2C+Tony+W\" title=\"Tony W. Tong\"><span class=\"author-style\">\n",
       " Tony W. Tong</span></a></li>\n",
       "<li><a href=\"/action/doSearch?ContribAuthorStored=Wu%2C+Shubin\" title=\"Shubin Wu\"><span class=\"author-style\">\n",
       " Shubin Wu</span></a></li>\n",
       "</ul>\n",
       "<ul class=\"rlist--inline separator issue-item__details\" style=\"float: none; list-style: none;\">\n",
       "<li class=\"page-range\" style=\"display: inline-block;margin-left: 0;\"><span tabindex=\"0\">Pages: </span><span tabindex=\"0\">127-152</span></li>\n",
       "<li class=\"ePubDate\" style=\"display: inline-block;margin-left: 0;\"><span tabindex=\"0\">First Published: </span><span tabindex=\"0\">09 October 2018</span></li>\n",
       "</ul>\n",
       "<div class=\"content-item-format-links\">\n",
       "<ul class=\"rlist--inline separator issue-item__details\">\n",
       "<li><a href=\"/doi/abs/10.1002/smj.2969\" title=\"Abstract\">Abstract</a></li>\n",
       "<li><a href=\"/doi/full/10.1002/smj.2969\" title=\"\n",
       "            Full text\n",
       "        \">\n",
       "                    Full text\n",
       "                </a></li>\n",
       "<li class=\"pdfLink--is-hidden\"><a href=\"/doi/pdf/10.1002/smj.2969\" title=\"PDF\">\n",
       "                    PDF\n",
       "                </a></li>\n",
       "<li class=\"readcubeEPdfLink\"><a href=\"/doi/epdf/10.1002/smj.2969\" title=\"PDF\">\n",
       "                    PDF\n",
       "                </a></li>\n",
       "<li><a href=\"/doi/full/10.1002/smj.2969#reference\" title=\"References\">References</a></li><li><a href=\"/servlet/linkout?type=rightslink&amp;url=publisherName=Wiley%26publication=smj%26contentID=10.1002%252Fsmj.2969\" target=\"_blank\">Request permissions</a></li>\n",
       "</ul>\n",
       "</div>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_ToC_soup =\\\n",
    "BeautifulSoup(sleep_get(smj_url + issue_url,\n",
    "                          headers = headers).content,\n",
    "             'lxml')\\\n",
    "                            .find_all('div', {'class' : 'card issue-items-container exportCitationWrapper'})\n",
    "issue_ToC_secs_soup = dict(zip([toc_sec.find('h3').attrs.get('title')\n",
    "                                        for toc_sec in issue_ToC_soup],\n",
    "                                 issue_ToC_soup))\n",
    "#\n",
    "# Identify the ToC sections for which constituent articles have abstracts.\n",
    "abstract_bearing_ToC_secs = ['articles', 'research notes and communication', \n",
    "                             'research articles', 'commentary',\n",
    "                             'SPECIAL ISSUE ARTICLES'.lower(),\n",
    "                             'introduction']\n",
    "#\n",
    "# Filter ToC sections, retaining only those for which abstracts are included.\n",
    "issue_ToC_secs_soup = {section : issue_item_container\n",
    "                         for (section, issue_item_container) in issue_ToC_secs_soup.items()\n",
    "                         if section.lower() in abstract_bearing_ToC_secs}\n",
    "#\n",
    "# We now go after article titles and abstract URLs.  The essential information \n",
    "# is contained in <div> tabs assigned `issue-item` class tags.  We want to collect\n",
    "# these from all of the ToC sections.  We collect the results in an enumerated\n",
    "# dictionary.  Since our issue ToC may have more than one abstract-beraing section,\n",
    "# our list-comprehension gives us a list of lists. We use itertools.chain to\n",
    "# flatten into single list.\n",
    "issue_item_soup = dict(enumerate(it.chain(*[issue_item.find_all('div', {'class' : 'issue-item'})\n",
    "                                                for issue_item in issue_ToC_secs_soup.values()])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article = np.random.choice(a = list(issue_item_soup.keys()),\n",
    "                           size = 1).item(0)\n",
    "article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volume : 1991 - Volume 12\n",
      "Issue : Volume 12, Issue S1\n"
     ]
    }
   ],
   "source": [
    "volume = np.random.choice(a = list(smj_volumes.keys()),\n",
    "                          size = 1).item(0)\n",
    "issue = np.random.choice(a = list(smj_volumes.get(volume).get('issues').keys()),\n",
    "                         size = 1).item(0)\n",
    "print(f'Volume : {volume},   Issue : {issue}')\n",
    "\n",
    "smj_volumes.get(volume).get('issues').get(issue)\n",
    "\n",
    "issue_attrs = smj_volumes.get(volume).get('issues').get(issue)\n",
    "issue_url = issue_attrs.get('href')\n",
    "print(issue_url)\n",
    "\n",
    "\n",
    "def get_article_title_abst_url(issue_url):\n",
    "    # We begin with issue_url, the url to the issue table-of-contents page.\n",
    "    # The ToC is in a table-like format of hierarchical <div>-type objects\n",
    "    # The delimiters of interest are the class labeled\n",
    "    #      `card issue-items-container exportCitationWrapper`.\n",
    "    # \n",
    "    # Now, multiple types of these `card issue-items container` occur. \n",
    "    # Not all of them contain abstracts.  We need to sort out the ones that\n",
    "    # interest us and leave the rest. The ToC sections are distinguished\n",
    "    # by <h3> tags, for which `title` is an attribute. We get these, and \n",
    "    # use them as keys, for which the corresponding values are the\n",
    "    # totality of the issue-items containers.\n",
    "    issue_ToC_soup =\\\n",
    "    BeautifulSoup(sleep_get(smj_url + issue_url,\n",
    "                              headers = headers).content,\n",
    "                 'lxml')\\\n",
    "                                .find_all('div', {'class' : 'card issue-items-container exportCitationWrapper'})\n",
    "    issue_ToC_secs_soup = dict(zip([toc_sec.find('h3').attrs.get('title')\n",
    "                                            for toc_sec in issue_ToC_soup],\n",
    "                                     issue_ToC_soup))\n",
    "    #\n",
    "    # Identify the ToC sections for which constituent articles have abstracts.\n",
    "    abstract_bearing_ToC_secs = ['articles', 'research notes and communication', \n",
    "                                 'research articles', 'commentary',\n",
    "                                 'SPECIAL ISSUE ARTICLES'.lower(),\n",
    "                                 'introduction']\n",
    "    #\n",
    "    # Filter ToC sections, retaining only those for which abstracts are included.\n",
    "    issue_ToC_secs_soup = {section : issue_item_container\n",
    "                             for (section, issue_item_container) in issue_ToC_secs_soup.items()\n",
    "                             if section.lower() in abstract_bearing_ToC_secs}\n",
    "    #\n",
    "    # We now go after article titles and abstract URLs.  The essential information \n",
    "    # is contained in <div> tabs assigned `issue-item` class tags.  We want to collect\n",
    "    # these from all of the ToC sections.  We collect the results in an enumerated\n",
    "    # dictionary.  Since our issue ToC may have more than one abstract-beraing section,\n",
    "    # our list-comprehension gives us a list of lists. We use itertools.chain to\n",
    "    # flatten into single list.\n",
    "    issue_item_soup = dict(enumerate(it.chain(*[issue_item.find_all('div', {'class' : 'issue-item'})\n",
    "                                                    for issue_item in issue_ToC_secs_soup.values()])))\n",
    "    #\n",
    "    # Next, we get the title and the abstract URL. The title is contained in an <h2>-tagged object.\n",
    "    # The URL to the abstract page is buried a bit deeper.  It resides in a <div>-tagged\n",
    "    # object for which the class label\n",
    "    #            `content-item-format-links`\n",
    "    # is assigned. This contains a list, the first element of which is the abstract.  We get\n",
    "    # the corresponding attribute from a <a>-labeled object.\n",
    "    issue_items = {article_idx : {'title' : item_issue.find('h2').text,\n",
    "                                   'abstract_url' : item_issue.find('div', {'class' : 'content-item-format-links'})\\\n",
    "                                                              .find('a')\\\n",
    "                                                              .attrs\\\n",
    "                                                              .get('href'),\n",
    "                                   'link_type'    : item_issue.find('div', {'class' : 'content-item-format-links'})\\\n",
    "                                                              .find('a')\\\n",
    "                                                              .attrs\\\n",
    "                                                              .get('title').lower()}\n",
    "                                    for (article_idx, item_issue) in issue_item_soup.items() } \n",
    "    #\n",
    "    # We need to filter our dictionary.  Sometimes an article contains something other than\n",
    "    # an abstract.  Here, we drop articles for which no abstract is provided.\n",
    "    issue_items = {issue_idx : issue_attrs\n",
    "                        for (issue_idx, issue_attrs) in issue_items.items()\n",
    "                        if issue_attrs.get('link_type') == 'abstract'}\n",
    "    #\n",
    "    # We finally get the abstracts themselves. We use locally-defined function \n",
    "    # get_article_abstract to scrape this text block out of the location specified\n",
    "    # by the corresponding URL\n",
    "    issue_abstracts = {article_idx : {'abstract' : get_article_abstract(attribute.get('abstract_url'))}\n",
    "                                        for (article_idx, attribute) in issue_items.items()}\n",
    "    #\n",
    "    # We now assemble all of the information by merging the dictionaries.\n",
    "    issue_title_abstr = {issue_idx : {'title' : issue_attrs.get('title').lower(),\n",
    "                                      'abstract_url' : issue_attrs.get('abstract_url'),\n",
    "                                      'abstract' : issue_abstracts.get(issue_idx).get('abstract').lower()}\n",
    "                         for (issue_idx, issue_attrs) in issue_items.items()}\n",
    "    return issue_title_abstr\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly-selected volumes ['1988 - Volume 9' '1996 - Volume 17']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'1996 - Volume 17': {'href': '/loi/10970266/year/1996',\n",
       "  'issues': {'Volume 17, Issue S2': {'href': '/toc/10970266/1996/17/S2',\n",
       "    'cover_date': 'Winter 1996'},\n",
       "   'Volume 17, Issue 9': {'href': '/toc/10970266/1996/17/9',\n",
       "    'cover_date': 'November 1996'},\n",
       "   'Volume 17, Issue 8': {'href': '/toc/10970266/1996/17/8',\n",
       "    'cover_date': 'October 1996'},\n",
       "   'Volume 17, Issue 7': {'href': '/toc/10970266/1996/17/7',\n",
       "    'cover_date': 'July 1996'},\n",
       "   'Volume 17, Issue 6': {'href': '/toc/10970266/1996/17/6',\n",
       "    'cover_date': 'June 1996'},\n",
       "   'Volume 17, Issue S1': {'href': '/toc/10970266/1996/17/S1',\n",
       "    'cover_date': 'Summer 1996'},\n",
       "   'Volume 17, Issue 5': {'href': '/toc/10970266/1996/17/5',\n",
       "    'cover_date': 'May 1996'},\n",
       "   'Volume 17, Issue 4': {'href': '/toc/10970266/1996/17/4',\n",
       "    'cover_date': 'April 1996'},\n",
       "   'Volume 17, Issue 3': {'href': '/toc/10970266/1996/17/3',\n",
       "    'cover_date': 'March 1996'},\n",
       "   'Volume 17, Issue 2': {'href': '/toc/10970266/1996/17/2',\n",
       "    'cover_date': 'February 1996'},\n",
       "   'Volume 17, Issue 1': {'href': '/toc/10970266/1996/17/1',\n",
       "    'cover_date': 'January 1996'}}},\n",
       " '1988 - Volume 9': {'href': '/loi/10970266/year/1988',\n",
       "  'issues': {'Volume 9, Issue 6': {'href': '/toc/10970266/1988/9/6',\n",
       "    'cover_date': 'November/December 1988'},\n",
       "   'Volume 9, Issue 5': {'href': '/toc/10970266/1988/9/5',\n",
       "    'cover_date': 'September/October 1988'},\n",
       "   'Volume 9, Issue 4': {'href': '/toc/10970266/1988/9/4',\n",
       "    'cover_date': 'July/August 1988'},\n",
       "   'Volume 9, Issue S1': {'href': '/toc/10970266/1988/9/S1',\n",
       "    'cover_date': 'Summer 1988'},\n",
       "   'Volume 9, Issue 3': {'href': '/toc/10970266/1988/9/3',\n",
       "    'cover_date': 'May/June 1988'},\n",
       "   'Volume 9, Issue 2': {'href': '/toc/10970266/1988/9/2',\n",
       "    'cover_date': 'March/April 1988'},\n",
       "   'Volume 9, Issue 1': {'href': '/toc/10970266/1988/9/1',\n",
       "    'cover_date': 'January/February 1988'}}}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smj_volumes_short = dict()\n",
    "rand_select_vols = np.random.choice(a = list(smj_volumes.keys()),\n",
    "                                    size = 2,\n",
    "                                    replace = False)\n",
    "print(f'Randomly-selected volumes {rand_select_vols}')\n",
    "smj_volumes_short = {volume : vol_attrs\n",
    "                        for (volume, vol_attrs) in smj_volumes.items()\n",
    "                         if volume in rand_select_vols}\n",
    "smj_volumes_short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "volume = np.random.choice(a = list(smj_volumes.keys()),\n",
    "                                    size = 1,\n",
    "                                    replace = False).item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 volumes started at 20-03-02, 172019Z\n",
      "Volume 17, Issue S2, 11 articles, success at 20-03-02, 172058Z\n",
      "Volume 17, Issue 9, 3 articles, success at 20-03-02, 172111Z\n",
      "Volume 17, Issue 8, 4 articles, success at 20-03-02, 172146Z\n",
      "Volume 17, Issue 7 failure at 20-03-02, 172149Z\n",
      "Volume 17, Issue 6, 4 articles, success at 20-03-02, 172207Z\n",
      "Volume 17, Issue S1, 10 articles, success at 20-03-02, 172243Z\n",
      "Volume 17, Issue 5, 4 articles, success at 20-03-02, 172259Z\n",
      "Volume 17, Issue 4, 4 articles, success at 20-03-02, 172312Z\n",
      "Volume 17, Issue 3, 0 articles, success at 20-03-02, 172318Z\n",
      "Volume 17, Issue 2, 0 articles, success at 20-03-02, 172320Z\n",
      "Volume 17, Issue 1, 5 articles, success at 20-03-02, 172344Z\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './data/smj_title_abstr.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3b6072028195>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     vol_attrs.update({'issue_done' : issue_done,\n\u001b[1;32m     19\u001b[0m                       'issue_missed' : issue_missed})\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/smj_title_abstr.json'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         json.dump(smj_vols_titles_abstracts, \n\u001b[1;32m     22\u001b[0m                   \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/smj_title_abstr.json'"
     ]
    }
   ],
   "source": [
    "smj_vols_titles_abstracts = deepcopy(smj_volumes_short)\n",
    "print('{} volumes started at {}'.format(str(len(smj_vols_titles_abstracts)), \n",
    "                                        datetime.datetime.utcnow().strftime('%y-%m-%d, %H%M%SZ')))\n",
    "for (vols, vol_attrs) in smj_vols_titles_abstracts.items():\n",
    "    issue_done = list()\n",
    "    issue_missed = list()\n",
    "    for (issue, issue_attrs) in vol_attrs.get('issues').items():\n",
    "        try:\n",
    "            issue_attrs.update({'issue_toc' : get_article_title_abst_url(issue_attrs.get('href'))})\n",
    "            issue_done.append((issue, datetime.datetime.utcnow().strftime('%H%M%SZ')))\n",
    "            print('{}, {} articles, success at {}'.format(issue, \n",
    "                                                          str(len(issue_attrs.get('issue_toc'))),\n",
    "                                                          datetime.datetime.utcnow().strftime('%y-%m-%d, %H%M%SZ')))\n",
    "\n",
    "        except:\n",
    "            issue_missed.append((issue, datetime.datetime.utcnow().strftime('%H%M%SZ')))\n",
    "            print('{} failure at {}'.format(issue, datetime.datetime.utcnow().strftime('%y-%m-%d, %H%M%SZ')))\n",
    "    vol_attrs.update({'issue_done' : issue_done,\n",
    "                      'issue_missed' : issue_missed})\n",
    "    with io.open('./data/smj_title_abstr.json', 'w', encoding = 'utf-8') as f:\n",
    "        json.dump(smj_vols_titles_abstracts, \n",
    "                  f, \n",
    "                  ensure_ascii = False, \n",
    "                  indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smj_vols_titles_abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "##🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑##\n",
    "# smj_titles_abstracts = {vol : {'href' : vol_attrs.get('href'),\n",
    "#                              'issues' : {issue : {'href' : iss_attr.get('href'),\n",
    "#                                                   'cover_date' : iss_attr.get('cover_date'),\n",
    "#                                                   'issue_toc' : get_article_title_abst_url(iss_attr.get('href'))}\n",
    "#                                          for (issue, iss_attr) in vol_attrs.get('issues').items()}}\n",
    "#                             for (vol, vol_attrs) in smj_vols_short.items()}\n",
    "##🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑🛑##\n",
    "##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# volume = np.random.choice(a = list(smj_volumes_short.keys()),\n",
    "#                           size = 1).item(0)\n",
    "# volume_attrs = smj_volumes_short.get(volume)\n",
    "\n",
    "# issue = np.random.choice(a = list(volume_attrs.get('issues').keys()),\n",
    "#                           size = 1).item(0)\n",
    "\n",
    "# issue_attrs = volume_attrs.get('issues').get(issue)\n",
    "\n",
    "\n",
    "\n",
    "# print(f'Volume {volume}\\nIssue {issue}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
